{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(11037)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\"temperature\": 0.7, \"top_p\": 0.9}\n",
    "\n",
    "class LanguageModel:    \n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 enable_thinking: bool = False,\n",
    "                 params: dict = None) -> None:\n",
    "        if params is None:\n",
    "            params = default_params.copy()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.enable_thinking = enable_thinking\n",
    "        self.params = params\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self._loaded = False\n",
    "    \n",
    "    def load(self) -> None:\n",
    "        if self._loaded:\n",
    "            return\n",
    "            \n",
    "        print(f\"  Loading model: {self.model_name} (thinking={'enabled' if self.enable_thinking else 'disabled'})\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        #self.model = self.model.to(\"cuda\")\n",
    "        self._loaded = True\n",
    "        print(f\"  Model loaded successfully\")\n",
    "    \n",
    "    def unload(self) -> None:\n",
    "        if not self._loaded:\n",
    "            return\n",
    "            \n",
    "        print(f\"  Unloading model: {self.model_name}\")\n",
    "        \n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._loaded = False\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"  Model unloaded, GPU memory freed\")\n",
    "\n",
    "    def update_params(self, params: dict) -> None:\n",
    "        self.params = params\n",
    "\n",
    "    def call(self, prompt: str) -> str:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=self.enable_thinking,\n",
    "        )\n",
    "        \n",
    "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        generated_ids = self.model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=8192 if self.enable_thinking else 2048,\n",
    "            temperature=self.params[\"temperature\"],\n",
    "            top_p=self.params[\"top_p\"]\n",
    "        )\n",
    "        \n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "        if self.enable_thinking:\n",
    "            try:\n",
    "                index = len(output_ids) - output_ids[::-1].index(151668) \n",
    "            except ValueError:\n",
    "                index = 0\n",
    "        else:\n",
    "            index = 0\n",
    "\n",
    "        return self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationResult:    \n",
    "    def __init__(self, \n",
    "                 name: str,\n",
    "                 description: str,\n",
    "                 input_type: str,\n",
    "                 example_inputs: List[str],\n",
    "                 edge_cases: List[str],\n",
    "                 maximize_coverage: str,\n",
    "                 raw_distillation: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.input_type = input_type\n",
    "        self.example_inputs = example_inputs\n",
    "        self.edge_cases = edge_cases\n",
    "        self.maximize_coverage = maximize_coverage\n",
    "        self.raw_distillation = raw_distillation\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"input_type\": self.input_type,\n",
    "            \"example_inputs\": self.example_inputs,\n",
    "            \"edge_cases\": self.edge_cases,\n",
    "            \"maximize_coverage\": self.maximize_coverage,\n",
    "            \"raw_distillation\": self.raw_distillation\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict[str, Any]) -> \"DistillationResult\":\n",
    "        return cls(\n",
    "            name=data.get(\"name\", \"Unknown\"),\n",
    "            description=data.get(\"description\", \"\"),\n",
    "            input_type=data.get(\"input_type\", \"string\"),\n",
    "            example_inputs=data.get(\"example_inputs\", []),\n",
    "            edge_cases=data.get(\"edge_cases\", []),\n",
    "            maximize_coverage=data.get(\"maximize_coverage\", \"\"),\n",
    "            raw_distillation=data.get(\"raw_distillation\", \"\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzTargetDistillator:\n",
    "    def __init__(self, language_model: LanguageModel, base_dir: str):\n",
    "        self.llm = language_model\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def _read_files(self, file_paths: List[str]) -> str:\n",
    "        file_contents = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            full_path = os.path.join(self.base_dir, file_path)\n",
    "            if os.path.exists(full_path):\n",
    "                try:\n",
    "                    with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        file_contents.append(f\"=== FILE: {file_path} ===\\n{content}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Could not read {file_path}: {e}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(file_contents)\n",
    "\n",
    "    def distillate(self, target_key: str, file_paths: List[str]) -> DistillationResult:        \n",
    "        source_code = self._read_files(file_paths)\n",
    "        \n",
    "        if not source_code:\n",
    "            return DistillationResult(\n",
    "                name=target_key,\n",
    "                description=\"No source files found\",\n",
    "                input_type=\"string\",\n",
    "                example_inputs=[],\n",
    "                edge_cases=[],\n",
    "                maximize_coverage=\"\",\n",
    "                raw_distillation=\"\"\n",
    "            )\n",
    "        \n",
    "        if len(source_code) > 32768:\n",
    "            source_code = source_code[:32768] + \"\\n\\n... (truncated)\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a code analysis expert. Your task is to analyze the following Go source code and prepare a CONCISE technical summary for a fuzzing input generator.\n",
    "\n",
    "=== SOURCE CODE ===\n",
    "{source_code}\n",
    "\n",
    "=== YOUR TASK ===\n",
    "Analyze this code and provide a structured JSON response with the following information:\n",
    "\n",
    "1. **name**: A short descriptive name for this fuzzing target (e.g., \"CEL Filter Parser\", \"Markdown Tag Extractor\")\n",
    "\n",
    "2. **description**: A CONCISE technical summary (max 500 words) that includes:\n",
    "   - What the code does (main functionality)\n",
    "   - Input format: exact syntax rules, grammar, allowed characters\n",
    "   - How inputs are processed\n",
    "   - Error handling patterns\n",
    "   - Any validation or parsing logic\n",
    "\n",
    "3. **input_type**: Either \"string\" or \"bytes\" - what type of input the main functions accept\n",
    "\n",
    "4. **example_inputs**: Array of 5-10 valid input examples that would be accepted by the code, make them different to show variety of inputs\n",
    "\n",
    "5. **edge_cases**: Array of 10-20 edge cases that might cause errors, panics, or unexpected behavior:\n",
    "   - Boundary conditions (empty, very long, special chars)\n",
    "   - Malformed inputs\n",
    "   - Unicode edge cases\n",
    "   - Security-relevant inputs (injections, bypasses)\n",
    "\n",
    "6. **maximize_coverage**: Strictly and technically explain which cases would lead to getting into different parts of code and maximizing code coverage\n",
    "\n",
    "Be CONCISE but TECHNICAL. Focus on details that help generate effective fuzz inputs.\n",
    "\n",
    "IMPORTANT: Return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "  \"name\": \"...\",\n",
    "  \"description\": \"...\",\n",
    "  \"input_type\": \"string\",\n",
    "  \"example_inputs\": [\"...\", \"...\"],\n",
    "  \"edge_cases\": [\"...\", \"...\"],\n",
    "  \"maximize_coverage\": \"...\"\n",
    "}}\n",
    "\n",
    "No markdown, no explanations inside or outside JSON.\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.call(prompt)\n",
    "        \n",
    "        try:\n",
    "            start = response.find('{')\n",
    "            end = response.rfind('}') + 1\n",
    "            if start >= 0 and end > start:\n",
    "                json_str = response[start:end]\n",
    "                data = json.loads(json_str)\n",
    "                \n",
    "                return DistillationResult(\n",
    "                    name=data.get(\"name\", target_key),\n",
    "                    description=data.get(\"description\", \"\"),\n",
    "                    input_type=data.get(\"input_type\", \"string\"),\n",
    "                    example_inputs=data.get(\"example_inputs\", []),\n",
    "                    edge_cases=data.get(\"edge_cases\", []),\n",
    "                    maximize_coverage=data.get(\"maximize_coverage\", \"\"),\n",
    "                    raw_distillation=response\n",
    "                )\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  Warning: Failed to parse JSON from distillation: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error processing distillation: {e}\")\n",
    "        \n",
    "        return DistillationResult(\n",
    "            name=target_key,\n",
    "            description=response,\n",
    "            input_type=\"string\",\n",
    "            example_inputs=[],\n",
    "            edge_cases=[],\n",
    "            maximize_coverage=\"\",\n",
    "            raw_distillation=response\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUZZ_FILE_GROUPS = {\n",
    "    \"filter\": {\n",
    "        \"files\": [\"plugin/filter/parser.go\", \"plugin/filter/engine.go\"],\n",
    "    },\n",
    "    \"url\": {\n",
    "        \"files\": [\"plugin/httpgetter/html_meta.go\"],\n",
    "    },\n",
    "    \"email\": {\n",
    "        \"files\": [\"internal/util/util.go\"],\n",
    "    },\n",
    "    \"uid\": {\n",
    "        \"files\": [\"internal/base/resource_name.go\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetedFuzzer:    \n",
    "    def __init__(self, language_model: LanguageModel):\n",
    "        self.llm = language_model\n",
    "\n",
    "    def _flatten_inputs(self, inputs) -> List[str]:\n",
    "        result = []\n",
    "        \n",
    "        if not isinstance(inputs, list):\n",
    "            if isinstance(inputs, str):\n",
    "                return [inputs]\n",
    "            return [str(inputs)]\n",
    "            \n",
    "        for item in inputs:\n",
    "            if isinstance(item, list):\n",
    "                result.extend(self._flatten_inputs(item))\n",
    "            elif isinstance(item, dict):\n",
    "                result.append(json.dumps(item, ensure_ascii=False))\n",
    "            elif isinstance(item, bytes):\n",
    "                result.append(item.decode('utf-8', errors='replace'))\n",
    "            elif isinstance(item, str):\n",
    "                result.append(item)\n",
    "            elif item is not None:\n",
    "                result.append(str(item))\n",
    "        return result\n",
    "\n",
    "    def generate_inputs(self, \n",
    "                        distillation: DistillationResult,\n",
    "                        existing_inputs: List[str],\n",
    "                        num_inputs: int = 20,\n",
    "                        iteration: int = 1) -> List[str]:\n",
    "        if existing_inputs:\n",
    "            max_show = min(50, len(existing_inputs))\n",
    "            recent_inputs = existing_inputs[-max_show:]\n",
    "            existing_inputs_section = f\"\"\"\n",
    "Previously generated inputs ({len(existing_inputs)} total, showing last {max_show}):\n",
    "{json.dumps(recent_inputs, indent=2, ensure_ascii=False)}\n",
    "\n",
    "IMPORTANT: Generate NEW and DIFFERENT inputs. Do not repeat the above. Don't try to copy patterns above.\n",
    "\"\"\"\n",
    "        else:\n",
    "            existing_inputs_section = \"\"\"\n",
    "This is the first iteration. Start with diverse inputs.\n",
    "\"\"\"\n",
    "\n",
    "        iteration_guidance = {\n",
    "            1: \"Focus on: basic valid inputs, simple variations, common patterns\",\n",
    "            2: \"Focus on: maximizing code coverage based on code structure analysis\", \n",
    "            3: \"Focus on: Unicode characters, special symbols, escape sequences\",\n",
    "            4: \"Focus on: malformed inputs, syntax errors, invalid combinations\",\n",
    "            5: \"Focus on: security payloads, injection attempts, bypass patterns\",\n",
    "            6: \"Focus on: boundary conditions, empty/null inputs, very long inputs\",\n",
    "        }\n",
    "        \n",
    "        guidance = iteration_guidance.get(iteration, \"Focus on: creative edge cases not yet covered\")\n",
    "        \n",
    "        examples_section = \"\"\n",
    "        if distillation.example_inputs:\n",
    "            examples_section = f\"\"\"\n",
    "=== VALID INPUT EXAMPLES (from code analysis) ===\n",
    "{json.dumps(distillation.example_inputs, indent=2, ensure_ascii=False)}\n",
    "\"\"\"\n",
    "        \n",
    "        edge_cases_section = \"\"\n",
    "        if distillation.edge_cases:\n",
    "            edge_cases_section = f\"\"\"\n",
    "=== KNOWN EDGE CASES ===\n",
    "{json.dumps(distillation.edge_cases, indent=2, ensure_ascii=False)}\n",
    "\"\"\"\n",
    "\n",
    "        coverage_section = \"\"\n",
    "        if distillation.maximize_coverage and iteration >= 5:\n",
    "            coverage_section = f\"\"\"\n",
    "=== CODE COVERAGE GUIDANCE ===\n",
    "{distillation.maximize_coverage}\n",
    "\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a fuzzing expert generating test inputs.\n",
    "\n",
    "=== TARGET ===\n",
    "Name: {distillation.name}\n",
    "Input type: {distillation.input_type}\n",
    "\n",
    "=== TECHNICAL CONTEXT ===\n",
    "{distillation.description}\n",
    "{examples_section}\n",
    "{edge_cases_section}\n",
    "{coverage_section}\n",
    "\n",
    "=== EXISTING CORPUS ===\n",
    "{existing_inputs_section}\n",
    "\n",
    "=== ITERATION {iteration} GUIDANCE ===\n",
    "{guidance}\n",
    "\n",
    "=== YOUR TASK ===\n",
    "Generate {num_inputs} NEW and UNIQUE test inputs.\n",
    "\n",
    "Requirements:\n",
    "1. Each input must be DIFFERENT from previously generated ones\n",
    "2. Cover new edge cases and patterns\n",
    "3. Include both valid and invalid inputs\n",
    "4. Be creative but relevant to the target\n",
    "\n",
    "CRITICAL: Return ONLY a valid JSON array of strings:\n",
    "[\"input1\", \"input2\", \"input3\", ...]\n",
    "\n",
    "No explanations, no markdown, just the JSON array.\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.call(prompt)\n",
    "        \n",
    "        try:\n",
    "            start = response.find('[')\n",
    "            end = response.rfind(']') + 1\n",
    "            if start >= 0 and end > start:\n",
    "                json_str = response[start:end]\n",
    "                inputs = json.loads(json_str)\n",
    "                return self._flatten_inputs(inputs)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"    Warning: JSON parse error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Error processing inputs: {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "    def generate_corpus(self, \n",
    "                        distillation: DistillationResult,\n",
    "                        iterations: int = 6, \n",
    "                        inputs_per_iter: int = 20) -> List[str]:\n",
    "        all_inputs_list = []\n",
    "        all_inputs_set = set()\n",
    "        \n",
    "        seed_inputs = distillation.example_inputs + distillation.edge_cases\n",
    "        for inp in seed_inputs:\n",
    "            if isinstance(inp, str) and inp not in all_inputs_set:\n",
    "                all_inputs_set.add(inp)\n",
    "                all_inputs_list.append(inp)\n",
    "        \n",
    "        print(f\"  Starting with {len(all_inputs_list)} seed inputs from distillation\")\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print(f\"  Iteration {i+1}/{iterations}...\")\n",
    "            try:\n",
    "                new_inputs = self.generate_inputs(\n",
    "                    distillation=distillation,\n",
    "                    existing_inputs=all_inputs_list,\n",
    "                    num_inputs=inputs_per_iter,\n",
    "                    iteration=i + 1\n",
    "                )\n",
    "                \n",
    "                added_count = 0\n",
    "                for inp in new_inputs:\n",
    "                    if isinstance(inp, str) and inp not in all_inputs_set:\n",
    "                        all_inputs_set.add(inp)\n",
    "                        all_inputs_list.append(inp)\n",
    "                        added_count += 1\n",
    "                \n",
    "                print(f\"    Generated {len(new_inputs)} inputs, added {added_count} new, total: {len(all_inputs_list)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error in iteration {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return all_inputs_list\n",
    "\n",
    "\n",
    "def save_corpus(corpus: List[str], target_key: str, output_dir: str = \"corpus\"):\n",
    "    target_dir = os.path.join(output_dir, target_key)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    for i, inp in enumerate(corpus):\n",
    "        file_path = os.path.join(target_dir, f\"input_{i:04d}\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            if isinstance(inp, str):\n",
    "                f.write(inp.encode('utf-8'))\n",
    "            else:\n",
    "                f.write(inp)\n",
    "    \n",
    "    print(f\"Saved {len(corpus)} inputs to {target_dir}\")\n",
    "\n",
    "\n",
    "def run_two_phase_pipeline(base_dir: str, \n",
    "                           file_groups: Dict[str, Dict] = None,\n",
    "                           distill_model_name: str = \"Qwen/Qwen3-1.7B\",\n",
    "                           gen_model_name: str = \"Qwen/Qwen3-1.7B\",\n",
    "                           iterations: int = 6,\n",
    "                           inputs_per_iter: int = 20):    \n",
    "    if file_groups is None:\n",
    "        file_groups = FUZZ_FILE_GROUPS\n",
    "    \n",
    "    results = {}\n",
    "    distillation_results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1: DISTILLATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    distill_model = LanguageModel(\n",
    "        model_name=distill_model_name,\n",
    "        enable_thinking=True,\n",
    "        params={\"temperature\": 0.6, \"top_p\": 0.95}\n",
    "    )\n",
    "    distill_model.load()\n",
    "    \n",
    "    distillator = FuzzTargetDistillator(distill_model, base_dir)\n",
    "    \n",
    "    for target_key, config in file_groups.items():\n",
    "        file_paths = config[\"files\"]\n",
    "        print(f\"\\n--- Distilling: {target_key} ---\")\n",
    "        print(f\"  Files: {file_paths}\")\n",
    "        \n",
    "        try:\n",
    "            distillation = distillator.distillate(target_key, file_paths)\n",
    "            distillation_results[target_key] = distillation\n",
    "            \n",
    "            print(f\"  Name: {distillation.name}\")\n",
    "            print(f\"  Input type: {distillation.input_type}\")\n",
    "            print(f\"  Examples: {len(distillation.example_inputs)}\")\n",
    "            print(f\"  Edge cases: {len(distillation.edge_cases)}\")\n",
    "            print(f\"  Description: {len(distillation.description)} chars\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            distillation_results[target_key] = DistillationResult(\n",
    "                name=target_key,\n",
    "                description=f\"Fuzzing target for files: {file_paths}\",\n",
    "                input_type=\"string\",\n",
    "                example_inputs=[],\n",
    "                edge_cases=[],\n",
    "                maximize_coverage=\"\",\n",
    "                raw_distillation=\"\"\n",
    "            )\n",
    "    \n",
    "    distill_model.unload()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 2: CORPUS GENERATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    gen_model = LanguageModel(\n",
    "        model_name=gen_model_name,\n",
    "        enable_thinking=False,\n",
    "        params={\"temperature\": 0.83, \"top_p\": 0.85}\n",
    "    )\n",
    "    gen_model.load()\n",
    "    \n",
    "    fuzzer = TargetedFuzzer(gen_model)\n",
    "    \n",
    "    for target_key in file_groups.keys():\n",
    "        distillation = distillation_results[target_key]\n",
    "        print(f\"\\n--- Generating corpus: {distillation.name} ({target_key}) ---\")\n",
    "        \n",
    "        try:\n",
    "            corpus = fuzzer.generate_corpus(\n",
    "                distillation=distillation,\n",
    "                iterations=iterations,\n",
    "                inputs_per_iter=inputs_per_iter\n",
    "            )\n",
    "            \n",
    "            save_corpus(corpus, target_key)\n",
    "            \n",
    "            results[target_key] = {\n",
    "                \"distillation\": distillation.to_dict(),\n",
    "                \"corpus_size\": len(corpus),\n",
    "                \"corpus\": corpus\n",
    "            }\n",
    "            print(f\"  Generated {len(corpus)} unique inputs\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            results[target_key] = {\"error\": str(e)}\n",
    "    \n",
    "    gen_model.unload()\n",
    "    \n",
    "    return results, distillation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, distillations = run_two_phase_pipeline(\n",
    "    base_dir=\"memos\",\n",
    "    file_groups=FUZZ_FILE_GROUPS,\n",
    "    distill_model_name=\"Qwen/Qwen3-4B\",\n",
    "    gen_model_name=\"Qwen/Qwen3-1.7B\",\n",
    "    iterations=6,\n",
    "    inputs_per_iter=20\n",
    ")\n",
    "\n",
    "with open(\"fuzzing_results.json\", \"w\") as f:\n",
    "    summary = {}\n",
    "    for k, v in results.items():\n",
    "        if \"error\" in v:\n",
    "            summary[k] = {\"error\": v[\"error\"]}\n",
    "        else:\n",
    "            summary[k] = {\n",
    "                \"name\": v[\"distillation\"][\"name\"],\n",
    "                \"input_type\": v[\"distillation\"][\"input_type\"],\n",
    "                \"corpus_size\": v[\"corpus_size\"],\n",
    "                \"sample_inputs\": v[\"corpus\"][:20]\n",
    "            }\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"distillation_results.json\", \"w\") as f:\n",
    "    distill_data = {k: v.to_dict() for k, v in distillations.items()}\n",
    "    json.dump(distill_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nDistillation Results:\")\n",
    "for target_key, distillation in distillations.items():\n",
    "    print(f\"  {target_key}:\")\n",
    "    print(f\"    Name: {distillation.name}\")\n",
    "    print(f\"    Type: {distillation.input_type}\")\n",
    "    print(f\"    Examples: {len(distillation.example_inputs)}, Edge cases: {len(distillation.edge_cases)}\")\n",
    "\n",
    "print(\"\\nCorpus Generation:\")\n",
    "total_inputs = 0\n",
    "for target, data in results.items():\n",
    "    if \"error\" in data:\n",
    "        print(f\"  {target}: ERROR - {data['error']}\")\n",
    "    else:\n",
    "        print(f\"  {target}: {data['corpus_size']} inputs\")\n",
    "        total_inputs += data['corpus_size']\n",
    "\n",
    "print(f\"\\nTotal inputs generated: {total_inputs}\")\n",
    "print(\"Corpus saved to: ./corpus/\")\n",
    "print(\"Results saved to: fuzzing_results.json\")\n",
    "print(\"Distillation saved to: distillation_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9316294,
     "sourceId": 14584235,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9316334,
     "sourceId": 14584294,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
